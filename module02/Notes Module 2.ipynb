{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes Module 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lession 12 - Videos\n",
    "\n",
    "1. Intro https://www.youtube.com/watch?v=C7vWJH05tKA\n",
    "2. Distributions https://www.youtube.com/watch?v=ZlRGxq5I9BU\n",
    "4. Parameters of a Distribution https://www.youtube.com/watch?v=-akdmiLDny4\n",
    "6. Testing for Normality https://www.youtube.com/watch?v=Sa1MJegyYfc\n",
    "9. Heteroskedasticity https://www.youtube.com/watch?v=wias9OZ1tU4\n",
    "10. Transforming Data https://www.youtube.com/watch?v=N8Fhq8wiQZU\n",
    "11. Linear Regression https://www.youtube.com/watch?v=GRY4eakMBJ8\n",
    "14. Multivariate Regression https://www.youtube.com/watch?v=WbCGVF7SAN0\n",
    "15. Regression in Trading https://www.youtube.com/watch?v=bcOGRWxg7qQ\n",
    "17. Summary https://www.youtube.com/watch?v=n2VxcEcw0GY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 12-06 - Testing for normality\n",
    "\n",
    "There are some visual ways to check if a distribution is normally distributed or not. Recall that normal distributions are symmetric and do not have fat tails (a more formal term for “fat tails” is kurtosis”). **Box-whisker plots** helps us visually check if a distribution is symmetric or skewed. A **histogram** lets us check if a distribution is symmetric/skewed, and if it has fat tails. **QQ plots** help us compare any two distributions, so they can be used to compare distributions other than the normal distribution. If you plot the actual data’s distribution against a theoretical normal distribution, you can decide if the distributions are the same type if the QQ plot produces a fairly straight line.\n",
    "\n",
    "There are three hypothesis tests that can be used to decide if a data distribution is normal. These are the Shapiro-Wilk test and D’Agostino-Pearson, and the Kolmogorov-Smirnov test. Each of these produce p-value, and if the p-value is small enough, say 0.05 or less, we can say with a 95% confidence that the data is not normally distributed. **Shapiro-Wilk** tends to perform better in a broader set of cases compared to the **D’Agostino-Pearson** test. In part, this is because the D’Agostino-Pearson test is used to look for skewness and kurtosis that do not match a normal distribution, so there are some odd non-normal distributions for which it doesn’t detect non-normality, where the Sharpiro-Wilk would give the correct answer.\n",
    "\n",
    "The **Kolmogorov Smirnov** test can be used to compare distributions other than the normal distribution, so it’s similar to the QQ plot in its generality. To do a normality test, we would first rescale the data distribution (subtract the mean and divide by its standard deviation), then compare the rescaled data distribution with the standard normal distribution (which has a mean of zero and standard deviation of 1). In general, the Shapiro-WIlk test tends to be a better test than the Kolmogorov Smirnov test, but not in all cases.\n",
    "\n",
    "So in summary, if you want to be thorough, you can use all three tests (there are even more tests that we haven’t discussed here). If you only want to use one test, use the Shapiro-Wilk test. For a sanity check, visualize your data distribution with a histogram, box-whisker plot, and/or a QQ plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 12-09 - Homoskedasticity\n",
    "\n",
    "We also need to check if our data is **stationary** over time. By stationary, we mean that the mean, variance, and covariance are the same over time. In particular, we want to check that the variance of our data is stable over time. This is important because if the variance changes over time, then the tests that we use to validate our model will be incorrect. The term for constant variance over time is **homoscedasticity**. The term for a changing variance over time is **heteroskedasticity**.\n",
    "\n",
    "One of the assumptions of linear regression is that its input data are homoscedastic. A visual way to check if the our data is homoscedastic is a **scatter plot**. If our data is heteroscedastic, a linear regression estimate of the coefficients may be less accurate (further from the actual value), and we may get a smaller p-value than should be expected, which means we may assume (incorrectly) that we have an accurate estimate of the regression coefficient, and assume that it’s statistically significant when it’s not. One way to check for homoskedasticity is the **Breusch-Pagan Test** which we will see later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 12-10 - Transforming Data\n",
    "\n",
    "So, now the question is, what do we do when our data is not normally distributed? Similarly, what do we do when our data is heteroscedastic? To reshape our data and make it more normal, we can feed our data into the log function.\n",
    "To get data that is homoscedastic, we can take the time difference between periods. By time difference, I mean that we can view our data as the rate of return from one day to the next. Similarly, we could get the time difference by\n",
    "subtracting each previous day's value from the current day's value. In practice, we take the rate of change for each period and then apply the log function. You may have seen this earlier when learning about why we model financial data with log returns.\n",
    "\n",
    "A way to make our data both normally distributed and homoscedastic, is by applying the **Box-Cox transformation**.\n",
    "To preview what the Box-Cox transformation does, let's get a conceptual idea of what a transformation looks like.\n",
    "Imagine you place all your data points on a number line, they are not evenly spaced and have odd clusters in\n",
    "some parts and no data points in other places. Think of this as a necklace in which the beads are not spaced out in a very nice way. Now, imagine if we could nudge each of these beads a little bit to the left and to the right to even out the spacing in a nicer way. Notice that since these beads are all on the same string, the relative order of each data point remains the same, we're just spacing them out in a way that's easier to work with. In math terms, we just applied a monotonic transformation. A monotonic transformation changes the values of a dataset but preserves the relative order. \n",
    "\n",
    "The Box-Cox transformation takes a dataset and outputs a dataset that is more normally distributed.\n",
    "You can see that the Box-Cox transformation has one constant value, lambda. If you choose lambda to be zero, then the transformation function is defined as the natural log. You can try different values for lambda to transform the data,\n",
    "then perform tests for normality and homoscedasticity. Once you can say your data is normally distributed, you can use it in the various models that we'll discuss in the rest of the lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 12-11 - Linear Regression \n",
    "\n",
    "We can check if the **residuals** follow a normal distribution. If the residuals follow a normal distribution with a mean of zero and a constant standard deviation, then these residuals can be considered random. By random, I mean that the model's predicted value is equally likely to be higher or lower than the actual value. If however, the average of the residuals is not zero, this gives us a hint that the model has a bias in its prediction errors. \n",
    "\n",
    "One way to improve our model is to look for other independent variables. This is called multiple regression, when we use more than one independent variable to predict a dependent variable. \n",
    "\n",
    "Now that we fit a regression model, we want to check if we can rely on it for future predictions. One measure of our model's ability to fit the data is the **R-squared** value. The R-squared is a metric that ranges from zero to one. R-squared of one means that all the variation in the dependent variable, can be explained by all the variation in the independent variables. \n",
    "\n",
    "A better metric is the **adjusted R-squared**, which helps us to find the minimum combination of independent variables that are most relevant for our model. \n",
    "\n",
    "Another way to check our model, is by performing an **F-test**. The F-test checks whether our coefficients and intercepts are not zero, and therefore, meaningful. If we get a P-value of 0.05 or smaller, we can assume that our parameters are not zero. When parameters are not zero, then we can say that our model describes a meaningful relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 12-12 - Breusch Pagan in Depth\n",
    "\n",
    "### Breusch-Pagan Test for Heteroscedasticity (revisited)\n",
    "Now that we’ve covered regression, let’s look at the Breusch Pagan test in more depth. The Breusch-Pagan test is one of many tests for homoscedasticity/heteroscedasticity. It takes the residuals from a regression, and checks if they are dependent upon the independent variables that we fed into the regression. (Note that we’ll explain residuals in a few videos within this lesson, so feel free to jump back here after you watch the video “Linear Regression”). The test does this by performing a second regression of the residuals against the independent variables, and checking if the coefficients from that second regression are statistically significant (non-zero). If the coefficients of this second regression are significant, then the residuals depend upon the independent variables. If the residuals depend upon the independent variables, then it means that the variance of the data depends on the independent variables. In other words, the data is likely heteroscedastic. So if the p-value of the Breusch-Pagan test is ≤ 0.05, we can assume with a 95% confidence that the distribution is heteroscedastic (not homoscedastic).\n",
    "\n",
    "### Breusch-Pagan Test in Python\n",
    "In Python, we can use the [`statsmodels.stats.diagnostic.het_breuschpagan(resid, exog_het)`](http://www.statsmodels.org/dev/generated/statsmodels.stats.diagnostic.het_breuschpagan.html) function to test for heteroscedasticity. We input the residuals from the regression of the dependent variable against the independent variables. We also input the independent variables that may affect the variance of the data. The function outputs a p-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 13 - Videos\n",
    "\n",
    "1. Time Series Modeling https://www.youtube.com/watch?v=QeIu7GMZl20\n",
    "2. Autoreressive Models https://www.youtube.com/watch?v=9jE7S4b-oIU\n",
    "3. Moving Average Models https://www.youtube.com/watch?v=1FkCP_dwxjI\n",
    "4. Advanced Time Series Models https://www.youtube.com/watch?v=cj1RCBTDog8\n",
    "5. Kalman Filters https://www.youtube.com/watch?v=CLJhgfMI4Ho\n",
    "6. Particle Filter https://www.youtube.com/watch?v=4KhDUAvwI74\n",
    "7. Recurrent Neural Networks https://www.youtube.com/watch?v=5cYAAHyRHDo\n",
    "8. Summary https://www.youtube.com/watch?v=6sheR92KUU8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 13-01 - Time Series Modeling \n",
    "\n",
    "Welcome to the lesson on time series analysis. Time series are data that are collected at regular intervals. We will cover two statistical methods, autoregression and moving averages. This will give us the foundation to cover two more advanced methods, autoregressive moving averages and autoregressive integrated moving averages. From there, we will cover two machine-learning methods. The first is Kalman filters, and the more generalized, particle filters. The second is recurrent neural networks. Let's get started.\n",
    "\n",
    "Let's think a bit about what a stock price time series looks like. Most stock prices increase over time. Sadly, some stock prices also decrease over time. This means that the price shows a trend. This makes analyzing the prices difficult since the prices are non-stationary. By non-stationary, we mean that the data's mean and standard deviation change over time. \n",
    "\n",
    "The goal of time series analysis is to use past data to predict future values. So, if the properties of the data change over time, the past is less useful in predicting the future. To work with data that is more likely to be stationary, and therefore, easier to model, we use stock returns and not stock prices. Moreover, to work with data that is more stationary and more normally distributed, we use the log of stock returns, which we call log returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 13-04 - Advanced Time Series Modeling \n",
    "\n",
    "Recall that regression-based time series models require the data to be stationary. When data is not stationary, the mean, variance, or co-variance may change over time, and it's hard to use the past to predict the future. \n",
    "\n",
    "One way to get a stationary time series is by taking the difference between points in the time series. The time difference may also be called the rate of change or the item wise difference. We call that the rate of change between periods, or the rate of return, can be calculated by taking the ratio of the current price divided by the previous price. When you express your data in terms of logs, this ratio becomes a difference between your current log price and the previous log price. \n",
    "\n",
    "When working with financial data, we usually find that asset price time series have a property such that their time difference is stationary. In other words, we like working with returns and not prices because the time series are more stable.\n",
    "\n",
    "In math terms, we say that the original price data is integrated of order one $I(1)$. We also say that the log returns of this data is integrated of order zero $I(0)$. So, when working the time series, you can check if it is stationary using a statistical test called the **Augmented Dickey Fuller** test. If the Augmented Dickey Fuller test gives a p-value that is 0.05 or less, then we can assume that the time series is stationary. If the data is not stationary, we can take the time difference then run the Augmented Dickey-Fuller test to see if the time difference is stationary. If it is stationary, then we can say that this time difference is integrated of order zero $I(0)$. We can also say that the original time series is integrated of order one $I(1)$. \n",
    "\n",
    "You may need to take the time difference multiple times. Then, once you find a time difference that is stationary, you refer to the original data as integrated of order $D$ where $D$ is the number of times that you had to take the time difference. \n",
    "\n",
    "Also, once we have a stationary time series, we can model it with an autoregressive moving average. Note that being familiar with the integrated order one $I(1)$ and integrated ordered zero $I(0)$ time series will help you as you learn about co-integration and pairs trading.\n",
    "\n",
    "### Seasonal Adjustments using ARIMA (SARIMA)\n",
    "Time series data tends to have seasonal patterns. For instance, natural gas prices may increase during winter months, when it’s used for heating homes. Similarly, it may also increase during peak summer months, when natural gas generators are used to produce the extra electricity that is used for air conditioning. Retail sales also has expected increases during the holiday shopping season, such as Black Friday in the US (November), and Singles’ Day in China (also in November).\n",
    "\n",
    "Stocks may potentially have seasonal patterns as well. One has to do with writing off losses in order to minimize taxes. Funds and individual investors have unrealized capital gains or losses when the stock price increases or decreases from the price at which they bought the stock. Those capital gains or losses become “realized capital gains” or “realized capital losses” when they sell the stock. At the end of the tax year (which may be December, but not necessarily), an investor may decide to sell their underperforming stocks in order to realize capital losses, which may potentially reduce their taxes. Then, at the start of the next tax year, they may buy back the same stocks in order to maintain their original portfolio. This is sometimes referred to as the “January effect.”\n",
    "\n",
    "Removing seasonal effects can help to make the resulting time series stationary, and therefore more useful when feeding into an autoregressive moving average model.\n",
    "\n",
    "To remove seasonality, we can take the difference between each data point and another data point one year prior. We’ll refer to this as the “seasonal difference”. For instance, if you have monthly data, take the difference between August 2018 and August 2017, and do the same for the rest of your data. It’s common to take the “first difference” either before or after taking the seasonal difference. If we took the “first difference” from the original time series, this would be taking August 2018 and subtracting July 2018. Next, to take the seasonal difference of the first difference, this would mean taking the difference between (August 2018 - July 2018) and (August 2017 - July 2017).\n",
    "\n",
    "You can check if the resulting time series is stationary, and if so, run this stationary series through an autoregressive moving average model.\n",
    "\n",
    "### Side Note\n",
    "Kendall Lo, one of the subject matter experts of our course, recommends this book: **“Way of the Turtle: The Secret Methods that Turned Ordinary People into Legendary Traders”**. The book is about how a successful investor trained his students (his “turtles”) to follow his trend-following trading strategy. The book illustrates the concepts of using trading signals, back-testing, position sizing, and risk management. The story is also summarized in this article [Turtle Trading: A Market Legend](https://www.investopedia.com/articles/trading/08/turtle-trading.asp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 13-06 Kalman Filter \n",
    "\n",
    "### Kalman Filters for Pairs Trading\n",
    "One way Kalman Filters are used in trading is for choosing the hedge ratio in pairs trading. We will get into pairs trading and hedge ratios in lesson 13 of this module, but for now, imagine that there’s a magic number that you can estimate from a model, such as a regression model, based on time series data of two stocks.\n",
    "\n",
    "Every day when you get another data point, you can run another regression and get an updated estimate for this number. So do you take the most recent number every time? Do you take a moving average? If so, how many days will you average together? Will you give each day the same weight when taking the average?\n",
    "\n",
    "All of these kinds of decisions are meant to smooth an estimate of a number that is based on noisy data. The Kalman Filter is designed to provide this estimate based on both past information and new observations. So instead of taking a moving average of this estimate, we can use a Kalman Filter.\n",
    "\n",
    "The Kalman Filter takes the time series of two stocks, and generate its “smoothed” estimate for this magic number at each new time period. Kalman Filters are often used in control systems for vehicles such as cars, planes, rockets, and robots. They’re similar to the application in pairs trading because they take noisy indirect measurements at each new time period in order to estimate state variables (location, direction, speed) of a system .\n",
    "\n",
    "Kalman Filters are not used in this module’s project, but if you want to learn more, please check out the extracurricular content section: [**\"Machine Learning\": Introduction to Kalman Filters**](https://classroom.udacity.com/nanodegrees/nd880/parts/aed900bc-7c5c-42ae-b2fe-6447da911c06/modules/a63b1e95-2015-4bca-b71b-eed5212c0b2c/lessons/882eb101-8593-4301-a41a-c9be6b3f40c6/concepts/c5d7510a-d595-46f9-9280-2ca59925a9de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 13-07 - Particle Filter \n",
    "\n",
    "Next, we'll discuss the particle filter, a type of genetic algorithm that is also used for self-driving cars as well as time series. By genetic algorithm, I mean that we apply natural selection to improve our estimates. This will become more clear in a bit. Let's start with a thought experiment.\n",
    "\n",
    "Imagine that we can hire many little helpers, each with a certain view on where the stock returns are going based on market data. Each of these little helpers predict the stock return for the next day, and on the following day, you can see how correct they were. Each day, you pay more to the little helpers who are correct and pay less to the incorrect helpers.\n",
    "\n",
    "Over time, only the accurate helpers remain to make predictions. So by the process of natural selection, we find the helpers who are most accurate and average their predictions as your best estimate.\n",
    "\n",
    "Since this process looks like natural selection and evolution, particle filters are considered a type of genetic algorithm. The little helpers are called particles. These particles are individual models whose parameters are set randomly. When most particles make very similar predictions, this also shows more confidence in the average prediction. When there are significant changes in the data distribution at any point in time, the particles may make predictions that are more different from each other.\n",
    "\n",
    "In those cases, when the individual predictions are more spread out, we are less confident of the prediction. Particle filters are pretty good at handling a variety of data because they don't assume the data to be normally distributed. Particle filters also do not assume a linear relationship, so they can better fit non-linear data.\n",
    "\n",
    "For an introduction to particle filters, please check out Sebastian Thrun’s lesson in the free “Intro to Artificial Intelligence” course: [**“HMMs and Filters: Node 18 “Particle Filters”**](https://classroom.udacity.com/courses/cs271/lessons/48734405/concepts/487221790923)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 14 - Videos\n",
    "\n",
    "1. What is Volatility ? https://www.youtube.com/watch?v=brGVwpDSuG4\n",
    "2. Historical Volatility https://www.youtube.com/watch?v=BOPhsYLHkUU\n",
    "3. Annualized Volatility https://www.youtube.com/watch?v=yakh1pjP7uY\n",
    "6. Rolling Windows https://www.youtube.com/watch?v=4EuMKqeNXA0\n",
    "8. Exponentially Weighted Moving Average https://www.youtube.com/watch?v=VBPitTHzYRI\n",
    "10. Forecasting Volatility https://www.youtube.com/watch?v=82v4v_PKDAE\n",
    "11. Markets & Volatility https://www.youtube.com/watch?v=jEHJkZUX9s4\n",
    "12. Using Volatility for Equity Trading https://www.youtube.com/watch?v=Vh9ajVRedvY\n",
    "13. Breakout Strategies https://www.youtube.com/watch?v=9eamk40DMu0\n",
    "14. Summary https://www.youtube.com/watch?v=FMXL37CkTgg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 15 - Videos\n",
    "\n",
    "1. Intro https://www.youtube.com/watch?v=CQ6QGAxbUF8\n",
    "2. Mean Reversion https://www.youtube.com/watch?v=zQ08lFcZa_A\n",
    "3. Pairs Trading https://www.youtube.com/watch?v=7lEm_tFXcBk\n",
    "5. Quiz: Identify Pairs to Trade\n",
    "4. Finding Pairs to Trade https://www.youtube.com/watch?v=6hQtoElcnGM\n",
    "6. Cointegration https://www.youtube.com/watch?v=N4ZI5SyFMOc\n",
    "7. ADF and roots\n",
    "8. Clustering Stocks https://www.youtube.com/watch?v=LkgCK_qPqWE\n",
    "9. Trade Pairs of Stocks https://www.youtube.com/watch?v=i1yVMrgjtB0\n",
    "10. Exercise: finding pairs\n",
    "11. Variations of Pairs Trading and Mean Reversion Trading\n",
    "12. 3 or more stocks (optional)\n",
    "13. Details of the Johansen test (very optional)\n",
    "14. Summary https://www.youtube.com/watch?v=wuzha8SU2jw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 15-02 Mean Reversion \n",
    "\n",
    "### Drift and Volatility Model (optional)\n",
    "The drift and volatility model is also called a Brownian Motion model, and is a type of stochastic volatility model. First, let’s discuss how this relates to the finance industry. Stochastic volatility models are fundamental building blocks for estimating the price of options (calls, puts, swaps) and also bonds. Before creating a model of an option (like a call option, for instance), we first want a model for the movement of its underlying asset (the stock price itself). The movement of the stock price is what the drift and volatility model (brownian motion model) attempts to describe.\n",
    "\n",
    "The word Brownian Motion refers to the movements of molecules suspended in fluid, since this model was first used in physics and later adapted for finance. So it helps to imagine the stock price as a small particle, drifting through a glass of water, while it’s being bumped around by other particles and molecules. The word “stochastic” is another word for “random”. Stochastic volatility models attempt to represent the movement of a stock price when the volatility of its movements is random. Stochastic volatility models were used to improve upon the work of Black, Scholes and Merton, who came up with the first formula for pricing options.\n",
    "\n",
    "Now let’s revisit the drift and volatility model and describe what it means.\n",
    "\n",
    "$dp_t = p_t \\mu d_t + p_t \\sigma_t \\epsilon \\sqrt(d_t)$\n",
    "\n",
    "First, notice that $dp_t$ on the left is referring to the differential of the stock price at time $t$. This type of equation is called a differential equation, since it describes the change over time of some process, rather than the specific state (stock price) of that series.\n",
    "\n",
    "The term $p_t \\mu d_t$  is the drift term. First, notice that it depends on the value of the stock price at time $t$ ($p_t$). This means that if we compare the movements of two stocks, one that’s priced at $2 per share, and another that’s priced at $1000 per share, the series with the larger price per share is expected to drift (change) more in absolute dollar amounts compared to the other stock. The $\\mu$ term is the expected return of the stock (think average return). Think of the expected return as the expected percent change over a period of time. We usually estimate the expected future return based on historical returns. So if a stock is expected to have a larger percent change per day compared to another, we’d also expect it to drift more (change more) compared to the other stock. This term also includes $d_t$ , which is the change in time (how much time has passed). If we watched a stock over a period of day versus over one month, we would expect it to drift more over a month, as more time has passed.\n",
    "\n",
    "Now let’s look at the volatility term. Think of this as the random, bouncy part of the stock movement. This term includes the stock price $p_t$. It also includes the standard deviation of the stock $\\sigma_t$, which is a function of time. This is why this model is a type of stochastic volatility model, because it allows for a non-constant volatility that varies over time. If a stock series has higher volatility, this will result in a larger overall movement in stock price (a higher $dp_t$). The $\\epsilon$ is a white noise term, which means it’s a random number with a mean of zero and standard deviation of one. The white noise accounts for movements in the stock price that are not accounted for by the model. Finally, there’s the square root of the change in time. Note that the product $\\epsilon \\sqrt(d_t)$ is usually written as $dW_t$, and named a Wiener process.\n",
    "\n",
    "### Back to Mean Reversion\n",
    "Okay, stepping back a bit to relate this to mean reversion. The drift and volatility model is a way to describe phenomena that we observe in real life, such as stock prices. The model assumes that there is a constant drift term with some added randomness, so we can expect that a series will bounce around, but still revert back to its long-term mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 15-06 Cointegration \n",
    "\n",
    "### Cointegration\n",
    "A way to think about whether two stocks’ time series are cointegrated is to see if some linear combination of their time series forms a stationary series. In other words, let’s say $stock_1$ and $stock_2$ are non-stationary, but $w_1 \\times stock_1 + w_2 \\times stock_2$ is a stationary series. Then we can also say that $stock_1$ and $stock_2$ are cointegrated.\n",
    "\n",
    "### Hedge Ratio\n",
    "We can perform a regression where $stock_2$ is the dependent variable, and $stock_1$ is the independent variable (it doesn’t matter which you choose to be $x$ or $y$). Then the regression coefficient, which is our hedge ratio, is effectively $\\frac{w_1}{w_2}$. You can see how multiplying $stock_1$ by $\\frac{w_1}{w_2}$ is similar to multiply $stock_1$ by $w_1$ and $stock_2$ with $w_2$; in either case, we’re weighting each stock so that their linear combination produces a stationary series.\n",
    "\n",
    "### Augmented Dickey Fuller Test\n",
    "To check if two series are cointegrated, we can use the Augmented Dickey Fuller (ADF) Test. First, let’s get some intuition to see what the ADF test is doing. It’s trying to determine if the linear combination of the two series, (which is also a time series) is stationary.\n",
    "\n",
    "A series is stationary when its mean and covariance are constant, and also when the autocorrelation between one time period and another only depends on the time duration between them, and not the specific point in time of each observation.\n",
    "\n",
    "If you could represent a series as an AR(1) model $y_t = \\beta y_{t-1} + \\epsilon_t$, let’s think about what happens if the $\\beta$ is greater than one. We can imagine putting in a value for $y_{t-1}$ to get an estimate for $y_t$; then for the next day, we’ll use that value as $y_{t-1}$ to put into the model and estimate the new $y_t$. We’d end up having a series that trends in one direction, so its mean is not constant, and therefore it is not stationary.\n",
    "\n",
    "Next, if we had a $\\beta$ equal to one, then $y_t = y_{t-1} + \\epsilon_t$. We call this special case a random walk, and it means that the current price is equal to the previous price plus some white noise. Even though the mean of this series is constant, its covariance between one time period and another depends upon the point in time of the observations, so it is also not stationary.\n",
    "\n",
    "Finally, if we had a $\\beta$ of less than one, then we notice that $y_t$ depends upon less than 100\\% of the value of its previous value $y_{t-1}$, with some added random noise $\\epsilon_t$. The series doesn’t trend in a particular direction. Its variance is also constant, and its covariance between any two data points doesn’t depend on the point in time of the data point. You can think of the series like a bouncing rubber ball that’s being tapped lightly by random raindrops. Without the rain, the bouncing ball would have smaller and smaller bounces, and eventually stop bouncing. With random raindrops falling on the ball, some raindrops would make the ball bounce more, others would make the ball bounce less. So overall, the ball maintains a constant bounce height over time.\n",
    "\n",
    "So conceptually, the Augmented Dickey Fuller Test is a hypothesis test for which the null hypothesis is that a series is a random walk (its $\\beta$ is equal to one), and so the null hypothesis assumes that the series is not stationary. The alternate hypothesis is that $\\beta$ is less than one, and therefore it’s a stationary series. So if the ADF produces a p-value of 0.05 or less, we can say with a 95% confidence level that the series is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 15-07 ADF and roots\n",
    "\n",
    "### ADF and Roots of the Characteristic Equation ( very optional but kinda cool!)\n",
    "The previous explanation of ADF is usually done with a bit more added vocabulary. Normally, you might hear ADF explained as a test for the existence of a unit root. This next section will introduce some math vocabulary, but please remember the main idea of the previous section if this section starts to feel like too much.\n",
    "\n",
    "If we have an AR(p) model $y_t = \\beta_1 y_{t-1} + … + \\beta_{t-p} y_{t-p} + \\epsilon_t$ we can put all the terms that are not the white noise $\\epsilon$ to the left, like this: $y_t - \\beta_1 y_{t-1} - … - \\beta_{t-p} y_{t-p} = \\epsilon_t$.\n",
    "\n",
    "Then we set the left side of the equation equal to zero. What we have on the left is called the “characteristic equation”. You might recall from learning algebra that when we set an equation equal to zero, we usually are trying to solve for the roots (the values that make the equation equal to zero). Before we can solve for the roots of this equation, we need to rewrite it differently using something called backward shift notation.\n",
    "\n",
    "Backward shift notation looks like this: $B^n y_t = y_{t-n}$. So when we see $y_{t-1}$, we’ll replace it with $B^1 y_t$. If we see $y_{t-2}$, we’ll replace it with $B^2 y_t$. The nice thing about backward shift notation is that we can describe our lags in terms of $y_t$, which will come in handy in the part that’s coming up.\n",
    "\n",
    "So we can change this equation: $y_t - \\beta_1 y_{t-1} - … - \\beta_{t-p} y_{t-p} = 0$, into this: $y_t - \\beta_1 (B y_t) - … - \\beta_{t-p} (B^p y_t) = 0$.\n",
    "\n",
    "Notice how we can now factor out the $y_t$, so we have: $y_t (1 -\\beta_1 B - … - \\beta_{t-p} B^p) = 0$.\n",
    "\n",
    "Okay, let’s look at some examples to see what this means. We saw previously that an AR(1) model with a coefficient of one: $y_t = y_{t-1} + \\epsilon_t$ is called a random walk, and that a random walk is not stationary. If write the characteristic equation of the random walk, it looks like this: $y_t - y_{t-1} = \\epsilon_t = 0$. Next, we rewrite it with backward shift notation: $y_t - B y_t = 0$. Then we factor out the $y_t$ to get: $y_t (1 - B) = 0$. And we solve for B to get B = 1. The root equals one, and you might hear people say that the series has a unit root, or that its root “equals unity”.\n",
    "\n",
    "Next, let’s look at an AR(1) series where the $\\beta$ coefficient is less than one (let’s say $\\beta$ is $\\frac{1}{2}$). $y_t = \\frac{1}{2} y_{t-1} + \\epsilon_t$. The characteristic equation looks like this: $y_t - \\frac{1}{2} y_{t-1} = \\epsilon_t = 0$. In backward shift notation, it looks like: $y_t - \\frac{1}{2} B y_t = 0$. Factor out the $y_t$: $y_t(1 - \\frac{1}{2} B) = 0$.\n",
    "\n",
    "Solving for B is solving for the unit root of the characteristic equation. So we get $1 = \\frac{1}{2} B$, and so $B = 2$. Since the root is greater than one, we can say that the series is stationary.\n",
    "\n",
    "Note that for series with more than one lag, we can solve for more than one root.\n",
    "\n",
    "The Augmented Dickey Fuller Test has a null hypothesis that a series has a unit root. In other words, the null hypothesis is that a series is a random walk, which is not stationary. The alternate hypothesis is that the roots of the series are all greater than one, which suggests that the series is stationary. If the ADF gives a p-value of 0.05 or less, we reject the null hypothesis and can assume that the series is stationary.\n",
    "\n",
    "### Engle-Granger Test\n",
    "The Engle Granger Test is used to check whether two series are cointegrated. It involves two steps. First, calculate the hedge ratio by running a regression on one series against the other $y_t = \\beta x_t$. We call the $\\beta$ the “hedge ratio”.\n",
    "\n",
    "Second, we take $y_t - \\beta x_t$ to create a series that may be stationary. We’ll call this new series $z_t$. Then we use the ADF test to check if that series $z_t$ is stationary. If $z_t$ is stationary, we can assume that the x and y series are cointegrated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 15-11 Variations of Pairs Trading and Mean Reversion Trading\n",
    "\n",
    "### Variations of Pairs Trading or Mean Reversion Trading\n",
    "Note that it’s also possible to extend pairs trading to more than two stocks. We can identify multiple pairs and include these pairs in the same portfolio. We can also analyze stocks that are in the same industry. If we grouped the stocks within the same industry into a virtual portfolio and calculated the return of that industry, this portfolio return would represent the general expected movement of all stocks within the industry. Then, for each individual stock series, we can calculate the spread between its return and the portfolio return. We can assume that stocks within the same industry may revert towards the industry average. So when the spread between the single stock and the industry changes significantly, we can use that as a signal to buy or sell.\n",
    "\n",
    "### Cointegration with 2 or more stocks\n",
    "#### Generalizing the 2-stock pairs trading method\n",
    "We can extend cointegration from two stocks to three stocks using a method called the **Johansen** test. First let’s see an example of how this works with two stocks.\n",
    "\n",
    "The Johansen test gives us coefficients that we can multiply to each of the two stock series, so that a linear combination produces a number, and we can use it the same way we used the spread in the prior pairs trading method.\n",
    "\n",
    "$w_1 \\times stock_1 + w_2 \\times stock_2 = spread$\n",
    "\n",
    "In other words, if the first stock series moves up significantly relative to the second stock, we can see this by an increase in the “spread” beyond its historical average. We will assume that the spread will revert down towards its historical average, so we’ll short the first stock that is relatively high, and long the second stock that is relatively low.\n",
    "\n",
    "So far, this looks pretty much like what you did before, except instead of computing a hedge ratio to multiply to one stock, the Johansen test gives you one coefficient to multiply to each of the two stock series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 15-12 - 3 or more stokcs (optional)\n",
    "\n",
    "### Extending to 3 stocks (optional)\n",
    "Now let’s extend this concept to three stocks. If we analyze three stock series with the Johansen, we can determine whether all three stocks together have a cointegrated relationship, and that a linear combination of all three form a stationary series. Note that for the purpose of cointegration trading we use the original price series, and do not convert them to log returns. The Johansen test also lets us decide whether only two series are needed to form a stationary series, but for now, let’s assume that we find a trio of stocks that are cointegrated.\n",
    "\n",
    "The Johansen gives us three coefficients, one for each stock series. We take the linear combination to get a spread.\n",
    "\n",
    "$w_1 \\times stock_1 + w_2 \\times stock_2 + w_3 \\times stock_3 = spread$\n",
    "\n",
    "We get the historical average of the spread. Then we check if the spread deviates significantly from that average. For example, let’s say the spread increases significantly. So we check whether each of the three individual series moved up or down significantly to result in the change in spread. We short the series that are relatively high, and long the series that are relatively low. To determine how much to long or short, we again use the weights that are given by the Johansen test $(w_1, w_2, w_3)$.\n",
    "\n",
    "For example, let’s say the spread has gotten larger. Let’s also pretend that $w_1$ is 0.5, $w_2$ is 0.3, and $w_3$ is -0.1. Notice that the weights do not need to sum to 1. We’ll long or short the number of shares for each stock in these proportions. So for instance, if we traded 5 shares of $stock_1$, we’ll trade 3 shares of $stock_2$, and one share of $stock_3$.\n",
    "\n",
    "If we notice that $stock_1$ is higher than normal, $stock_2$ is lower than normal, and $stock_3$ is lower than normal, then let’s see whether we long or short a stock, and by how much.\n",
    "\n",
    "Since $stock_1$ is higher than usual (relative to the others), we short 5 shares of $stock_1$ because we expect it should revert by decreasing relative to the others.\n",
    "\n",
    "Since $stock_2$ is lower than normal, we long it by 3 shares, because we expect it to revert by increasing relative to the others.\n",
    "\n",
    "Since $stock_3$ is lower than normal, so we also long it by 1 share but notice that $w_3$ is a negative number (-0.1). \n",
    "\n",
    "Whenever we see a negative weight, it means we change a buy to a sell, or change a sell to a buy. So we long a -1 shares, which is actually shorting 1 share."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 15-13 - Details of Johansen Test (optional)\n",
    "\n",
    "### Details of the Johansen test (very optional)\n",
    "So you may be wondering how we get these coefficients, and also how we check whether three stocks have a cointegrated relationship. For a closer look, let’s introduce a bit of math. Recall from the lesson on time series, that a vector autoregression attempts to describe a stock’s current value based on not only its prior values, but also the prior values of other stocks. Let’s use two stocks as an example: Note, I’m using the variable names “IBM” and “GE” to refer to the price series of these stocks. The $\\mu$ refers to a historical average for each stock’s time series. The “e” refers to an error term for each stock.\n",
    "\n",
    "$IBM_{t} = \\mu_{IBM} + \\beta_{1,1} \\times IBM_{t-1} + \\beta_{1,2} \\times GE_{t-1} + e_{1,t}$\n",
    "\n",
    "$GE_{t} = \\mu_{GE} + \\beta_{2,1} \\times IBM_{t-1} + \\beta_{2,2} \\times GE_{t-1} + e_{2,t}$\n",
    "\n",
    "We normally use matrices to make this easier to work with, so the equations above can be written as:\n",
    "\n",
    "$\\begin{bmatrix} IBM_t\\\\ GE_t \\end{bmatrix} = \\begin{bmatrix} \\mu_{IBM} \\\\ \\mu_{GE} \\end{bmatrix} + \\begin{bmatrix} \\beta_{1,1} & \\beta_{1,2}\\\\ \\beta_{2,1} & \\beta_{2,2} \\end{bmatrix} \\begin{bmatrix} IBM_{t-1}\\\\ GE_{t-1} \\end{bmatrix} + \\begin{bmatrix} e_{1,t}\\\\ e_{2,t} \\end{bmatrix}$\n",
    "\n",
    "To make things simpler to write, we’ll write the 2 x 2 matrix of betas with a capital B, and we’ll denote the vector of the two stocks with a lowercase x. We’ll write the vector of \\muμ’s with a single \\muμ, and so on. So the vector autoregression with a lag of one is:\n",
    "\n",
    "$x_{t} = \\mu + B x_{t-1} + e_{t}$\n",
    "\n",
    "For a lag of p, this formula looks like $x_{t} = \\mu + B_1 x_{t-1} + ... + B_p x_{t-p}+ e_{t}$\n",
    "\n",
    "Now, if you recall from studying cointegrated time series, taking the time-wise difference may help us create a stationary series. So we’ll denote the timewise difference as: As $\\Delta x_{t} = x_{t} - x_{t-1}$.\n",
    "\n",
    "Next, we can define x_t in using a Vector Error Correction Model (VECM) like this: $\\Delta x_{t} = \\mu + Bx_{t-1} + C_1\\Delta x_{t-1} + C_p \\Delta x_{t-p} + e_t$\n",
    "\n",
    "Notice how the $B x_{t-1}$ term is just the vector of the previous periods’ values, and not the time-wise difference like the other terms. All of the subsequent terms to its right are time-wise differences. The Johansen test checks how many rows in the matrix B are needed to form a cointegrated series. To do this, it does some math, using an eigenvalue decomposition (let’s not worry about it for now), to determine how likely the matrix B has a rank of 0, or 1, 2, or 3, up to the number of stocks that we’re looking at (most likely 2 or 3).\n",
    "\n",
    "Here’s a quick refresher on what the rank of a matrix is.\n",
    "\n",
    "If you have three equations like this:\n",
    "\n",
    "$1x + 1y = 1$  \n",
    "$2x + 2y = 2$  \n",
    "$3x + 3y = 3$  \n",
    "\n",
    "What do you notice about all of these equations? It looks like you only need one of the equations to describe all three of them. In this case, we’d say that the rank is 1.\n",
    "\n",
    "Similarly, when the Johansen test checks whether the rank of matrix B is 0, 1, 2 or 3, let’s see what this means for us practically. If we were trying to see if 3 stocks were cointegrated, and the Johansen test estimated that the rank of matrix B was 3, then we’d assume that all three stocks form a cointegrated relationship. If, on the other hand, the Johansen test results showed that the rank of matrix B was likely 2, then only 2 of the 3 stocks are necessary to form a cointegrated relationship. So we’d want to try out all the pairs of stocks to see which two are cointegrated. If the rank was zero, then that means there was no cointegration among the stocks that we looked at.\n",
    "\n",
    "To determine the rank, the Johansen test actually does a hypothesis test on whether the rank is 0, 1, 2 or 3, up to the number of stocks there are in the test (probably 2 or 3). Looking at the t-statistic or p-value can let you decide with a certain level of confidence if at least two or even three of these stocks form a cointegrated series.\n",
    "\n",
    "Okay, we’re almost there! The Johansen test gives us a vector that we can use as the weights we assign to each stock. If you are curious, this is the largest eigenvector that’s generated by the eigenvalue decomposition. But again, let’s not worry about how to do eigenvalue decomposition, and just see how to use this vector of weights. These are the weights that we mentioned earlier when computing the linear combination of the stock prices, which is used in the same way as the spread.\n",
    "\n",
    "So if we get $w_1, w_2, w_3$ from the eigenvector $w$, we use these as weights on each stock, as we saw earlier:\n",
    "\n",
    "$w_1 \\times stock_1 + w_2 \\times stock_2 + w_3 \\times stock_3 = spread$\n",
    "\n",
    "To summarize, the Johansen test figures out whether a group of stocks is cointegrated, and if so, how to calculate a “spread” that we’ll keep track of for temporary deviations from its historical average. It also gives us the proportion of shares to trade for each stock."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
